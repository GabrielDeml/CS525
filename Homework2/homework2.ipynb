{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-28 17:40:09.584288: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-28 17:40:09.584351: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from transformers import BertTokenizer\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy\n",
    "from keras.utils.np_utils import to_categorical  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "reviews = pd.read_csv('Reviews.csv')\n",
    "reviews = reviews.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gabe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Clean the data\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "reviews_without_stopwords = [word for word in reviews['Text'].str.split() if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'thought',\n",
       " 'this',\n",
       " 'was',\n",
       " 'like',\n",
       " 'tuna',\n",
       " 'fish',\n",
       " 'but',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'just',\n",
       " 'like',\n",
       " 'the',\n",
       " 'taller',\n",
       " 'cans',\n",
       " 'of',\n",
       " 'salmon',\n",
       " 'with',\n",
       " 'bones',\n",
       " 'in',\n",
       " 'it',\n",
       " 'just',\n",
       " 'a',\n",
       " 'shorter',\n",
       " 'version.',\n",
       " 'The',\n",
       " 'salmon',\n",
       " 'is',\n",
       " 'good',\n",
       " 'if',\n",
       " 'you',\n",
       " 'can',\n",
       " 'get',\n",
       " 'pass',\n",
       " 'the',\n",
       " 'bones.<br',\n",
       " '/><br',\n",
       " '/>What',\n",
       " 'I',\n",
       " 'do',\n",
       " 'is',\n",
       " 'de-bone',\n",
       " 'seven',\n",
       " 'cans',\n",
       " 'at',\n",
       " 'a',\n",
       " 'time',\n",
       " 'and',\n",
       " 'freeze',\n",
       " 'them',\n",
       " 'because',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'headache',\n",
       " 'to',\n",
       " 'have',\n",
       " 'to',\n",
       " 'de-bone',\n",
       " 'a',\n",
       " 'can',\n",
       " 'each',\n",
       " 'and',\n",
       " 'every',\n",
       " 'time.',\n",
       " 'Since',\n",
       " 'this',\n",
       " 'works',\n",
       " 'for',\n",
       " 'me',\n",
       " 'I',\n",
       " 'will',\n",
       " 'continue',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'this',\n",
       " 'product.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_without_stopwords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set everything to lower case and remove punctuation\n",
    "reviews_without_stopwords_or_punctuation = []\n",
    "# reviews_without_stopwords_or_punctuation_full_list = []\n",
    "for st in reviews_without_stopwords:\n",
    "\ttmp = []\n",
    "\tfor word in st:\n",
    "\t\tif word not in punctuation:\n",
    "\t\t\ttmp.append(word.lower())\n",
    "\t\t\t# reviews_without_stopwords_or_punctuation_full_list.append(word.lower())\n",
    "\treviews_without_stopwords_or_punctuation.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', 'been', 'purchasing', 'large', 'french', 'vanilla', 'iced', \"coffee's\", 'from', 'dunkin', 'donuts', 'for', 'at', 'least', '12', 'years', 'now.', 'recently', \"i've\", 'become', 'frustrated', 'at', 'the', 'stupidity', 'of', 'the', 'workers', 'that', 'they', 'employ', 'and', 'their', 'inability', 'to', 'make', 'my', 'coffee', 'the', 'way', 'that', 'i', 'ask', 'for.', 'they', 'also', 'sometimes', 'give', 'me', 'decaf', 'without', 'telling', 'me', 'simply', 'because', 'they', 'are', 'out', 'of', 'regular,', 'too', 'lazy', 'to', 'make', 'more,', 'and', 'think', 'that', 'i', \"won't\", 'notice.', 'couple', 'that', 'with', 'the', 'fact', 'it', 'cost', 'me', '$2.75', 'for', 'every', 'iced', 'coffee', 'i', 'purchase', 'and', 'i', 'simply', 'have', 'had', 'enough.', 'i', 'found', 'out', 'about', 'this', 'on', 'a', 'deal', 'website', 'and', 'figured', 'hey', 'why', 'not.', 'it', 'tastes', 'great,', 'gives', 'me', 'a', 'good', 'pick', 'me', 'up,', 'and', \"doesn't\", 'have', 'a', 'lot', 'of', 'the', 'bad', 'things', 'that', 'are', 'in', 'energy', 'drinks.', 'sure', 'it', 'has', 'the', 'energy', 'blend', 'that', 'most', 'do', 'and', 'also', '(some', 'of', 'which', 'you', 'might', 'not', 'need)', 'but', 'other', 'then', 'that', 'it', 'has', 'all', 'natural', 'ingredients.', 'it', 'has', 'real', 'milk', 'and', 'real', 'coffee', 'and', 'i', \"don't\", 'miss', 'my', 'iced', 'coffee', 'one', 'bit.', 'i', 'order', '3', '12', 'packs', 'and', 'already', 'have', 'an', 'order', 'placed', 'for', 'when', 'they', 'are', 'back', 'in', 'stock.', 'at', '65', 'cents', 'a', 'can', '(after', 'the', 'coupon)', 'you', \"can't\", 'go', 'wrong.', 'only', 'problem', 'was', 'some', 'of', 'the', 'cans', 'of', 'the', 'second', 'shipment', 'were', 'pretty', 'dented', 'up', 'but', 'its', 'not', 'a', 'deal', 'breaker.', 'i', 'even', 'wrote', 'the', 'company', 'about', 'the', 'freshness', 'of', 'the', 'product', 'seeing', 'how', 'there', 'was', 'no', 'expiration', 'date', 'and', 'they', 'told', 'me', 'how', 'to', 'read', 'the', 'code', 'on', 'the', 'bottom', 'of', 'the', 'can.', 'turns', 'out', 'the', 'ones', 'i', 'received', 'had', 'a', 'born', 'on', 'date', 'of', '12/14/2010', 'that', 'is', 'perfectly', 'acceptable.']\n",
      "113691\n"
     ]
    }
   ],
   "source": [
    "# print(len(reviews_without_stopwords_or_punctuation))\n",
    "print(reviews_without_stopwords_or_punctuation[0])\n",
    "print(len(reviews_without_stopwords_or_punctuation))\n",
    "# print(reviews['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/gabe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/gabe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_string(word_list):\n",
    "\treturn ' '.join([lemmatizer.lemmatize(words) for words in word_list])\n",
    "\n",
    "with Pool(processes=8) as pool:\n",
    "\treviews_without_stopwords_or_punctuation_lemmatize = pool.map(lemmatize_string, reviews_without_stopwords_or_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_string_list(word_list):\n",
    "\treturn [lemmatizer.lemmatize(words) for words in word_list]\n",
    "\n",
    "with Pool(processes=8) as pool:\n",
    "\treviews_without_stopwords_or_punctuation_lemmatize_list = pool.map(lemmatize_string_list, reviews_without_stopwords_or_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'the',\n",
       " '3rd',\n",
       " 'time',\n",
       " 'i',\n",
       " 'have',\n",
       " 'purchased',\n",
       " 'this',\n",
       " 'product',\n",
       " 'because',\n",
       " 'i',\n",
       " 'wa',\n",
       " 'very',\n",
       " 'pleased',\n",
       " 'with',\n",
       " 'the',\n",
       " 'taste',\n",
       " 'and',\n",
       " 'texture',\n",
       " 'of',\n",
       " 'the',\n",
       " 'oatmeal.',\n",
       " 'just',\n",
       " 'to',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'the',\n",
       " 'price',\n",
       " 'wa',\n",
       " 'good',\n",
       " 'i',\n",
       " 'shopped',\n",
       " 'in',\n",
       " '3',\n",
       " 'health',\n",
       " 'food',\n",
       " 'store',\n",
       " 'and',\n",
       " 'the',\n",
       " 'price',\n",
       " 'wa',\n",
       " 'almost',\n",
       " 'double.',\n",
       " 'i',\n",
       " 'keep',\n",
       " 'it',\n",
       " 'in',\n",
       " 'the',\n",
       " 'freezer',\n",
       " 'and',\n",
       " 'it',\n",
       " 'keep',\n",
       " 'until',\n",
       " 'i',\n",
       " 'order',\n",
       " 'again.',\n",
       " 'if',\n",
       " 'you',\n",
       " 'like',\n",
       " 'steel',\n",
       " 'cut',\n",
       " 'oat',\n",
       " 'you',\n",
       " 'will',\n",
       " 'love',\n",
       " 'this.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_without_stopwords_or_punctuation_lemmatize_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True, stop_words=None,\n",
    "                     ngram_range=(1, 1), tokenizer=token.tokenize)\n",
    "reviews_tokenized = cv.fit_transform(reviews_without_stopwords_or_punctuation_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113691, 59407)\n"
     ]
    }
   ],
   "source": [
    "print(reviews_tokenized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_score = [int(i) for i in reviews['Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(reviews_tokenized, review_score, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65693 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(reviews['Text'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (113691, 250)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(reviews['Text'])\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.asarray(reviews['Score'])\n",
    "y_numpy = numpy.array(reviews['Score'])\n",
    "y_one_hot  = np.zeros((y_numpy.size, y_numpy.max()+1))\n",
    "y_one_hot[np.arange(y_numpy.size),y_numpy] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90952, 250) (90952, 6)\n",
      "(22739, 250) (22739, 6)\n"
     ]
    }
   ],
   "source": [
    "x_train_lstm, x_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X, y_one_hot, test_size=0.2, random_state=0)\n",
    "print(x_train_lstm.shape,y_train_lstm.shape)\n",
    "print(x_test_lstm.shape,y_test_lstm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 250, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_9 (Spatial (None, 250, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 5,081,006\n",
      "Trainable params: 5,081,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(x_train_lstm, y_train_lstm, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier \n",
    "# from sklearn import metrics\n",
    "# clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "# predicted= clf.predict(X_test)\n",
    "# print(\"Decision Tree Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Precision: \" + str(metrics.precision_score(y_test, predicted, average='macro')))\n",
    "# print(\"Recall: \" + str(metrics.recall_score(y_test, predicted, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# clf = SVC().fit(X_train, y_train)\n",
    "# predicted= clf.predict(X_test)\n",
    "# print(\"SVM Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Precision: \" + str(metrics.precision_score(y_test, predicted, average='macro')))\n",
    "# print(\"Recall: \" + str(metrics.recall_score(y_test, predicted, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as gensim_api\n",
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_words = [remove_stopwords(i) for i in reviews['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I thought like tuna fish not. It like taller cans salmon bones shorter version. The salmon good pass bones.<br /><br />What I de-bone seven cans time freeze headache de-bone time. Since works I continue buy product.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stop_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3043/630988312.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenized_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_stop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenized_words = tokenizer.fit_on_texts(no_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_string_list(word_list):\n",
    "\treturn [lemmatizer.lemmatize(words) for words in word_list.split()]\n",
    "\n",
    "with Pool(processes=8) as pool:\n",
    "\treviews_without_stopwords_or_punctuation_lemmatize_list = pool.map(lemmatize_string_list, no_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = gensim_api.load(\"glove-twitter-25\")\n",
    "evbeddings = gensim_api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I thought like tuna fish not. It like taller cans salmon bones shorter version. The salmon good pass bones.<br /><br />What I de-bone seven cans time freeze headache de-bone time. Since works I continue buy product.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_without_stopwords_or_punctuation_lemmatize_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reviews_without_stopwords_or_punctuation_lemmatize_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3043/3492259562.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreviews_without_stopwords_or_punctuation_lemmatize_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mtmp_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reviews_without_stopwords_or_punctuation_lemmatize_list' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model_vector = []\n",
    "for text in reviews_without_stopwords_or_punctuation_lemmatize_list:\n",
    "\ttmp_array = []\n",
    "\tfor word in text:\n",
    "\t\ttry:\n",
    "\t\t\ttmp_array.append(evbeddings[word])\n",
    "\t\texcept:\n",
    "\t\t\tprint(word)\n",
    "\tmodel_vector.append(tmp_array)\n",
    "# \tprint(text)\n",
    "# \tprint(evbeddings.wv.most_similar(positive=text))\n",
    "# model_vector = (np.mean([evbeddings[token] for token in text for text in reviews_without_stopwords_or_punctuation_lemmatize_list], axis=0)).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using bert\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/gabe/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "reviews_tokenized_bert = tokenizer.batch_encode_plus(reviews_without_stopwords_or_punctuation_lemmatize, max_length=512, pad_to_max_length=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1045, 2031,  ...,    0,    0,    0],\n",
      "        [ 101, 1045, 2109,  ...,    0,    0,    0],\n",
      "        [ 101, 2023, 1037,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 1045, 2066,  ...,    0,    0,    0],\n",
      "        [ 101, 1045, 2293,  ...,    0,    0,    0],\n",
      "        [ 101, 1045, 4156,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "print(reviews_tokenized_bert['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(reviews_tokenized_bert.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews_tokenized_bert['input_ids'], review_score, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"SVM Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Precision: \" + str(metrics.precision_score(y_test, predicted, average='macro')))\n",
    "print(\"Recall: \" + str(metrics.recall_score(y_test, predicted, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Support Vector Machine Accuracy: 0.4991864198073794\n",
      "Precision: 0.21536380155123752\n",
      "Recall: 0.21034048696583216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabe/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Linear Support Vector Machine\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(max_iter=1000).fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"Linear Support Vector Machine Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Precision: \" + str(metrics.precision_score(y_test, predicted, average='macro')))\n",
    "print(\"Recall: \" + str(metrics.recall_score(y_test, predicted, average='macro')))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
