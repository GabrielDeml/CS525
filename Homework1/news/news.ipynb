{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import nltk\n",
    "import string\n",
    "import scipy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from multiprocessing import Pool, TimeoutError"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gabe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gabe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gabe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# import the Fake.csv and True.csv files\n",
    "true = pd.read_csv('True.csv')\n",
    "fake = pd.read_csv('Fake.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Combine all of the true data into one list and tokesnize it\n",
    "true_string = ''.join(true[\"text\"])\n",
    "true_tokenized = nltk.word_tokenize(true_string)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Combine all of the fake data into one list and tokesnize it\n",
    "fake_string = ''.join(fake[\"text\"])\n",
    "fake_tokenized = nltk.word_tokenize(fake_string)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make all the words lowercase and remove the punctuation\n",
    "true_tokenized = [word.lower() for word in true_tokenized if word.isalnum() or (not \".\" and not \",\" and not \"?\" and not \"!\")]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make all the words lowercase and remove the punctuation\n",
    "fake_tokenized = [word.lower() for word in fake_tokenized if word.isalnum() or (not \".\" and not \",\" and not \"?\" and not \"!\")]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Print the 100 most common words in the true data\n",
    "fdist_true = FreqDist(true_tokenized)\n",
    "print(\"True: \" + str(fdist_true.most_common(100)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Print the 100 most common words in the fake data\n",
    "fdist_fake = FreqDist(fake_tokenized)\n",
    "print(\"Fake: \" + str(fdist_fake.most_common(100)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot the 100 most common words in the true data\n",
    "fig = plt.figure(figsize = (100,20))\n",
    "fdist_true.plot(100,cumulative=False)\n",
    "fig.savefig('true_freq.png', bbox_inches = \"tight\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot the 100 most common words in the fake data\n",
    "fig = plt.figure(figsize = (100,20))\n",
    "fdist_fake.plot(100,cumulative=False)\n",
    "fig.savefig('fake_freq.png', bbox_inches = \"tight\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Remove the stop words from the true data\n",
    "stop_words = stopwords.words('english')\n",
    "true_tokenized = [word for word in true_tokenized if word not in stop_words]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Remove the stop words from the fake data\n",
    "fake_tokenized = [word for word in fake_tokenized if word not in stop_words]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Print the 100 most common words in the true data\n",
    "fdist_true = FreqDist(true_tokenized)\n",
    "print(\"True: \" + str(fdist_true.most_common(100)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Print the 100 most common words in the fake data\n",
    "fdist_fake = FreqDist(fake_tokenized)\n",
    "print(\"Fake: \" + str(fdist_fake.most_common(100)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Lemmatize the words in the true data\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "with Pool(processes=8) as pool:\n",
    "\ttrue_tokenized = pool.map(lemmatizer.lemmatize, true_tokenized)\n",
    "# true_tokenized = [lemmatizer.lemmatize(word) for word in true_tokenized]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Lemmatize the words in the fake data\n",
    "with Pool(processes=8) as pool:\n",
    "\tfake_tokenized = pool.map(lemmatizer.lemmatize, fake_tokenized)\n",
    "\n",
    "# fake_tokenized = [lemmatizer.lemmatize(word) for word in fake_tokenized]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Print the 100 most common words in the true data\n",
    "fdist_true = FreqDist(true_tokenized)\n",
    "print(\"True: \" + str(fdist_true.most_common(100)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Print the 100 most common words in the fake data\n",
    "fdist_fake = FreqDist(fake_tokenized)\n",
    "print(\"Fake: \" + str(fdist_fake.most_common(100)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set the true and fake labels\n",
    "true['label']=0\n",
    "fake['label']=1\n",
    "# Convert the true and fake data into a dataframe\n",
    "true_pandas = pd.DataFrame(true.to_numpy())\n",
    "fake_pandas = pd.DataFrame(fake.to_numpy())\n",
    "# Combine the true and fake data into one dataframe\n",
    "data = pd.concat([true_pandas,fake_pandas])\n",
    "# Print the dataframe\n",
    "print(data[4])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# Remove unwanted elements from out data like symbols and numbers and tokenize the data\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "data_clean = cv.fit_transform(data[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(data[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a dataset from the tokenized data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_clean, data[4].astype(\"int\"), test_size=0.2, random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a TF-IDF vectorizer\n",
    "tf=TfidfVectorizer()\n",
    "text_tf= tf.fit_transform(data[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a dataset from the TF-IDF vectorized data\n",
    "X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(text_tf, data[4].astype(\"int\"), test_size=0.3, random_state=123)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Raw Dataset\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Precision: \" + str(metrics.precision_score(y_test, predicted, average='macro')))\n",
    "print(\"Recall: \" + str(metrics.recall_score(y_test, predicted, average='macro')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Tfidf dataset\n",
    "clf = MultinomialNB().fit(X_train_tf, y_train_tf)\n",
    "predicted= clf.predict(X_test_tf)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test_tf, predicted))\n",
    "print(\"Precision: \" + str(metrics.precision_score(y_test_tf, predicted, average='macro')))\n",
    "print(\"Recall: \" + str(metrics.recall_score(y_test_tf, predicted, average='macro')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Raw dataset\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Precision: \" + str(metrics.precision_score(y_test, predicted, average='macro')))\n",
    "print(\"Recall: \" + str(metrics.recall_score(y_test, predicted, average='macro')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Tfidf dataset\n",
    "clf = LogisticRegression().fit(X_train_tf, y_train_tf)\n",
    "predicted= clf.predict(X_test_tf)\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test_tf, predicted))\n",
    "print(\"Precision: \" + str(metrics.precision_score(y_test_tf, predicted, average='macro')))\n",
    "print(\"Recall: \" + str(metrics.recall_score(y_test_tf, predicted, average='macro')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Classification using SVM\n",
    "# Raw datset\n",
    "clf = SVC().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"SVM Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Precision: \" + str(metrics.precision_score(y_test, predicted, average='macro')))\n",
    "print(\"Recall: \" + str(metrics.recall_score(y_test, predicted, average='macro')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Tfidf dataset\n",
    "clf = SVC().fit(X_train_tf, y_train_tf)\n",
    "predicted= clf.predict(X_test_tf)\n",
    "print(\"SVM Accuracy:\",metrics.accuracy_score(y_test_tf, predicted))\n",
    "print(\"Precision: \" + str(metrics.precision_score(y_test_tf, predicted, average='macro')))\n",
    "print(\"Recall: \" + str(metrics.recall_score(y_test_tf, predicted, average='macro')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Raw dataset\n",
    "clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Precision: \" + str(metrics.precision_score(y_test, predicted, average='macro')))\n",
    "print(\"Recall: \" + str(metrics.recall_score(y_test, predicted, average='macro')))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Tfidf dataset\n",
    "clf = RandomForestClassifier().fit(X_train_tf, y_train_tf)\n",
    "predicted= clf.predict(X_test_tf)\n",
    "print(\"Random Forest Accuracy:\",metrics.accuracy_score(y_test_tf, predicted))\n",
    "print(\"Precision: \" + str(metrics.precision_score(y_test_tf, predicted, average='macro')))\n",
    "print(\"Recall: \" + str(metrics.recall_score(y_test_tf, predicted, average='macro')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using just Nouns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "lemmatizer = WordNetLemmatizer()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def lemmatize(p : list):\n",
    "\ttoken = nltk.word_tokenize(p)\n",
    "\tpos = nltk.pos_tag(token)\n",
    "\tlem = [(word[1], lemmatizer.lemmatize(word[0])) for word in pos if word[1] == \"NN\"]\n",
    "\treturn lem"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "with Pool(processes=8) as pool:\n",
    "\tpos_tag_true = pool.map(lemmatize, true['text'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "with Pool(processes=8) as pool:\n",
    "\tpos_tag_fake = pool.map(lemmatize, fake['text'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def join_nouns(p : list):\n",
    "\treturn \" \".join([word[1] for word in p])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "pos_tag_nouns = pos_tag_true + pos_tag_fake"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "with Pool(processes=8) as pool:\n",
    "\tnouns_joined = pool.map(join_nouns, pos_tag_nouns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "nouns_labels = [0] * len(true) + [1] * len(fake)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "nouns_tmp = cv.fit_transform(nouns_joined)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# Create dataset split with nouns\n",
    "X_train_nouns, X_test_nouns, y_train_nouns, y_test_nouns = train_test_split(nouns_tmp, nouns_labels, test_size=0.3, random_state=123)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# Nouns dataset\n",
    "clf = MultinomialNB().fit(X_train_nouns, y_train_nouns)\n",
    "predicted= clf.predict(X_test_nouns)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test_nouns, predicted))\n",
    "print(\"Precision: \" + str(metrics.precision_score(y_test_nouns, predicted, average='macro')))\n",
    "print(\"Recall: \" + str(metrics.recall_score(y_test_nouns, predicted, average='macro')))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MultinomialNB Accuracy: 0.9184112843355605\n",
      "Precision: 0.9177836130165402\n",
      "Recall: 0.9187189461293814\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# POS Tagging dataset\n",
    "clf = LogisticRegression().fit(X_train_nouns, y_train_nouns)\n",
    "predicted= clf.predict(X_test_nouns)\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test_nouns, predicted))\n",
    "print(\"Precision: \" + str(metrics.precision_score(y_test_nouns, predicted, average='macro')))\n",
    "print(\"Recall: \" + str(metrics.recall_score(y_test_nouns, predicted, average='macro')))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logistic Regression Accuracy: 0.9536748329621381\n",
      "Precision: 0.9541114941525077\n",
      "Recall: 0.9528774814562144\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/gabe/.local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# POS Tagging dataset\n",
    "clf = SVC().fit(X_train_nouns, y_train_nouns)\n",
    "predicted= clf.predict(X_test_nouns)\n",
    "print(\"SVM Accuracy:\",metrics.accuracy_score(y_test_nouns, predicted))\n",
    "print(\"Precision: \" + str(metrics.precision_score(y_test_nouns, predicted, average='macro')))\n",
    "print(\"Recall: \" + str(metrics.recall_score(y_test_nouns, predicted, average='macro')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# POS Tagging dataset\n",
    "clf = RandomForestClassifier().fit(X_train_nouns, y_train_nouns)\n",
    "predicted= clf.predict(X_test_nouns)\n",
    "print(\"Random Forest Accuracy:\",metrics.accuracy_score(y_test_nouns, predicted))\n",
    "print(\"Precision: \" + str(metrics.precision_score(y_test_nouns, predicted, average='macro')))\n",
    "print(\"Recall: \" + str(metrics.recall_score(y_test_nouns, predicted, average='macro')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using Nouns and Verbs"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}