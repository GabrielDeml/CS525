{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 20:55:58.652466: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-26 20:55:58.652482: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import scipy\n",
    "from transformers import BertTokenizer\n",
    "from multiprocessing import Pool, TimeoutError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "reviews = pd.read_csv('Reviews.csv')\n",
    "reviews = reviews.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "reviews_without_stopwords = [word for word in reviews['Text'].str.split() if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set everything to lower case and remove punctuation\n",
    "reviews_without_stopwords_or_punctuation = []\n",
    "# reviews_without_stopwords_or_punctuation_full_list = []\n",
    "for st in reviews_without_stopwords:\n",
    "\ttmp = []\n",
    "\tfor word in st:\n",
    "\t\tif word not in punctuation:\n",
    "\t\t\ttmp.append(word.lower())\n",
    "\t\t\t# reviews_without_stopwords_or_punctuation_full_list.append(word.lower())\n",
    "\treviews_without_stopwords_or_punctuation.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'think', 'chai', 'tea', 'is', 'hard', 'to', 'find', 'with', 'a', 'smooth,', 'not', 'overly', 'done', 'flavor.', 'however,', 'leave', 'it', 'to', 'twinings', 'to', 'do', 'the', 'job', 'just', 'perfectly,', 'and', 'in', 'decaf!', 'i', 'purchased', 'this', 'tea', 'thinking,', '\"here', 'i', 'go', 'again,\"', 'hoping', 'for', 'the', 'perfect', 'cup', 'of', 'spice', 'and', 'tea.', 'this', 'one', 'fills', 'the', 'bill', 'and', 'then', 'some!', 'even', 'my', 'british', 'girlfriend', 'likes', 'this', 'one', '(hey,', 'twinings', 'is', 'from', 'london!).', \"it's\", 'not', 'over-spiced', 'like', 'so', 'many', \"chai's\", 'are', 'and', 'you', 'can', 'even', 'enjoy', 'this', 'one', 'without', 'sugar', 'and', 'milk', 'if', 'you', 'like', 'without', 'curling', 'your', 'hair', 'from', 'the', 'inside', 'out!', \"it's\", 'a', 'great', 'tea,', 'one', 'that', \"i'm\", 'going', 'to', 'be', 'buying', 'for', 'a', 'long', 'time', 'too', 'highly', 'recommend!']\n",
      "113691\n"
     ]
    }
   ],
   "source": [
    "# print(len(reviews_without_stopwords_or_punctuation))\n",
    "print(reviews_without_stopwords_or_punctuation[0])\n",
    "print(len(reviews_without_stopwords_or_punctuation))\n",
    "# print(reviews['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize_string(word_list):\n",
    "\treturn ' '.join([lemmatizer.lemmatize(words) for words in word_list])\n",
    "\n",
    "with Pool(processes=8) as pool:\n",
    "\treviews_without_stopwords_or_punctuation_lemmatize = pool.map(lemmatize_string, reviews_without_stopwords_or_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sometimes it's hard to find good quality product for the k-cups. this is a good tea. i love green tea and this is a good brand. i like this brand and bigelow organic green tea. i think the bigelow is my slight favorite, but this is good tea. no aftertaste and nor are there ground in the bottom of the cup. the price on this one put everything to shame. usually the k-cups are more expensive, so this is a bargain. get it now.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_without_stopwords_or_punctuation_lemmatize[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(lowercase=True, stop_words=None,\n",
    "                     ngram_range=(1, 1), tokenizer=token.tokenize)\n",
    "reviews_tokenized = cv.fit_transform(reviews_without_stopwords_or_punctuation_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113691, 58823)\n"
     ]
    }
   ],
   "source": [
    "print(reviews_tokenized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_score = [int(i) for i in reviews['Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews_tokenized, review_score, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier \n",
    "# from sklearn import metrics\n",
    "# clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "# predicted= clf.predict(X_test)\n",
    "# print(\"Decision Tree Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Precision: \" + str(metrics.precision_score(y_test, predicted, average='macro')))\n",
    "# print(\"Recall: \" + str(metrics.recall_score(y_test, predicted, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# clf = SVC().fit(X_train, y_train)\n",
    "# predicted= clf.predict(X_test)\n",
    "# print(\"SVM Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Precision: \" + str(metrics.precision_score(y_test, predicted, average='macro')))\n",
    "# print(\"Recall: \" + str(metrics.recall_score(y_test, predicted, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using bert\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/gabe/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "reviews_tokenized_bert = tokenizer.batch_encode_plus(reviews_without_stopwords_or_punctuation_lemmatize, max_length=512, pad_to_max_length=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1045, 2228,  ...,    0,    0,    0],\n",
      "        [ 101, 2823, 2009,  ...,    0,    0,    0],\n",
      "        [ 101, 1045, 3984,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2057, 2074,  ...,    0,    0,    0],\n",
      "        [ 101, 2025, 4840,  ...,    0,    0,    0],\n",
      "        [ 101, 2292, 2033,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "print(reviews_tokenized_bert['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(reviews_tokenized_bert.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews_tokenized_bert['input_ids'], review_score, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "# from sklearn.svm import SVC\n",
    "# clf = SVC().fit(X_train, y_train)\n",
    "# predicted= clf.predict(X_test)\n",
    "# print(\"SVM Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Precision: \" + str(metrics.precision_score(y_test, predicted, average='macro')))\n",
    "# print(\"Recall: \" + str(metrics.recall_score(y_test, predicted, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Support Vector Machine Accuracy: 0.4991864198073794\n",
      "Precision: 0.21536380155123752\n",
      "Recall: 0.21034048696583216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabe/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Linear Support Vector Machine\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(max_iter=1000).fit(X_train, y_train)\n",
    "predicted= clf.predict(X_test)\n",
    "print(\"Linear Support Vector Machine Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Precision: \" + str(metrics.precision_score(y_test, predicted, average='macro')))\n",
    "print(\"Recall: \" + str(metrics.recall_score(y_test, predicted, average='macro')))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
